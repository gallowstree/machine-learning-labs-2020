{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proyecto Final "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import graphviz\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import export_graphviz, DecisionTreeClassifier\n",
    "import pandas as pd\n",
    "from sklearn import svm\n",
    "from math import sqrt, pi, exp \n",
    "\n",
    "sns.set(style=\"darkgrid\")\n",
    "if tf.__version__.startswith(\"2.\"):\n",
    "  import tensorflow.compat.v1 as tf\n",
    "  tf.compat.v1.disable_v2_behavior()\n",
    "  tf.compat.v1.disable_eager_execution()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Carga de datos\n",
    "all_data = pd.read_csv('data_titanic_proyecto.csv')\n",
    "# train_set, test_set = train_test_split(all_data, test_size=0.2)\n",
    "# train_set, validation_set = train_test_split(train_set, test_size=0.2)\n",
    "# \n",
    "# print(\"all_data shape\", all_data.shape)\n",
    "# print(\"train_set shape\", train_set.shape)\n",
    "# print(\"validation_set shape\", validation_set.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análisis exploratorio y limpieza de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Buscaremos valores faltantes en todo el dataset para saber si se espera tener que realizar imputación. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "PassengerId             0\nName                    0\nAge                   177\nSibSp                   0\nParch                   0\nTicket                  0\nFare                    0\nCabin                 687\nEmbarked                2\npassenger_class         0\npassenger_sex           0\npassenger_survived      0\ndtype: int64"
     },
     "metadata": {},
     "execution_count": 147
    }
   ],
   "source": [
    "all_data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que Cabin tiene muchos valores faltantes, por lo que sería difícil realizar imputación, pero podemos utilizar estadística descriptiva para rellenar los valores faltantes en edad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos una muestra del data set para familiarizarnos con las variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "     PassengerId                                    Name   Age  SibSp  Parch  \\\n659          660              Newell, Mr. Arthur Webster  58.0      0      2   \n664          665             Lindqvist, Mr. Eino William  20.0      1      0   \n872          873                Carlsson, Mr. Frans Olof  33.0      0      0   \n533          534  Peter, Mrs. Catherine (Catherine Rizk)   NaN      0      2   \n\n                Ticket      Fare        Cabin Embarked  passenger_class  \\\n659              35273  113.2750          D48        C                1   \n664  STON/O 2. 3101285    7.9250          NaN        S                3   \n872                695    5.0000  B51 B53 B55        S                1   \n533               2668   22.3583          NaN        C                3   \n\n    passenger_sex  passenger_survived  \n659             M                   0  \n664             M                   1  \n872             M                   0  \n533             F                   1  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PassengerId</th>\n      <th>Name</th>\n      <th>Age</th>\n      <th>SibSp</th>\n      <th>Parch</th>\n      <th>Ticket</th>\n      <th>Fare</th>\n      <th>Cabin</th>\n      <th>Embarked</th>\n      <th>passenger_class</th>\n      <th>passenger_sex</th>\n      <th>passenger_survived</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>659</th>\n      <td>660</td>\n      <td>Newell, Mr. Arthur Webster</td>\n      <td>58.0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>35273</td>\n      <td>113.2750</td>\n      <td>D48</td>\n      <td>C</td>\n      <td>1</td>\n      <td>M</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>664</th>\n      <td>665</td>\n      <td>Lindqvist, Mr. Eino William</td>\n      <td>20.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>STON/O 2. 3101285</td>\n      <td>7.9250</td>\n      <td>NaN</td>\n      <td>S</td>\n      <td>3</td>\n      <td>M</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>872</th>\n      <td>873</td>\n      <td>Carlsson, Mr. Frans Olof</td>\n      <td>33.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>695</td>\n      <td>5.0000</td>\n      <td>B51 B53 B55</td>\n      <td>S</td>\n      <td>1</td>\n      <td>M</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>533</th>\n      <td>534</td>\n      <td>Peter, Mrs. Catherine (Catherine Rizk)</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>2</td>\n      <td>2668</td>\n      <td>22.3583</td>\n      <td>NaN</td>\n      <td>C</td>\n      <td>3</td>\n      <td>F</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 158
    }
   ],
   "source": [
    "all_data.sample(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SibSp es la suma del número de hermanos y cónyugues a bordo para el pasajero. \n",
    "\n",
    "Parch es la cantidad de padres e hijos a bordo. \n",
    "\n",
    "Embarked es la inicial del puerto donde embarcaró el pasajero. \n",
    "\n",
    "Vemos que la clase del pasajero es categórica (Upper, Lower, Middle) pero nos interesa su correlación con otras variables, por lo que la cambiaremos a una categórica numérica. Lo mismo con la variable objetivo, passenger_survived y passenger_sex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.replace({\"Upper\" : 1, \"Middle\" : 2, \"Lower\" : 3}, inplace=True)\n",
    "all_data.replace({\"Y\" : 1, \"N\" : 0}, inplace=True)\n",
    "all_data.replace({\"M\" : 1, \"F\" : 0}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que hemos transformado esas variables, podemos analizar la correlación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                    PassengerId       Age     SibSp     Parch      Fare  \\\nPassengerId            1.000000  0.036847  0.057527  0.001652  0.012658   \nAge                    0.036847  1.000000  0.308247  0.189119  0.096067   \nSibSp                  0.057527  0.308247  1.000000  0.414838  0.159651   \nParch                  0.001652  0.189119  0.414838  1.000000  0.216225   \nFare                   0.012658  0.096067  0.159651  0.216225  1.000000   \npassenger_class        0.035144  0.369226  0.083081  0.018443  0.549500   \npassenger_sex          0.042939  0.093254  0.114631  0.245489  0.182333   \npassenger_survived     0.005007  0.077221  0.035322  0.081629  0.257307   \n\n                    passenger_class  passenger_sex  passenger_survived  \nPassengerId                0.035144       0.042939            0.005007  \nAge                        0.369226       0.093254            0.077221  \nSibSp                      0.083081       0.114631            0.035322  \nParch                      0.018443       0.245489            0.081629  \nFare                       0.549500       0.182333            0.257307  \npassenger_class            1.000000       0.131900            0.338481  \npassenger_sex              0.131900       1.000000            0.543351  \npassenger_survived         0.338481       0.543351            1.000000  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PassengerId</th>\n      <th>Age</th>\n      <th>SibSp</th>\n      <th>Parch</th>\n      <th>Fare</th>\n      <th>passenger_class</th>\n      <th>passenger_sex</th>\n      <th>passenger_survived</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>PassengerId</th>\n      <td>1.000000</td>\n      <td>0.036847</td>\n      <td>0.057527</td>\n      <td>0.001652</td>\n      <td>0.012658</td>\n      <td>0.035144</td>\n      <td>0.042939</td>\n      <td>0.005007</td>\n    </tr>\n    <tr>\n      <th>Age</th>\n      <td>0.036847</td>\n      <td>1.000000</td>\n      <td>0.308247</td>\n      <td>0.189119</td>\n      <td>0.096067</td>\n      <td>0.369226</td>\n      <td>0.093254</td>\n      <td>0.077221</td>\n    </tr>\n    <tr>\n      <th>SibSp</th>\n      <td>0.057527</td>\n      <td>0.308247</td>\n      <td>1.000000</td>\n      <td>0.414838</td>\n      <td>0.159651</td>\n      <td>0.083081</td>\n      <td>0.114631</td>\n      <td>0.035322</td>\n    </tr>\n    <tr>\n      <th>Parch</th>\n      <td>0.001652</td>\n      <td>0.189119</td>\n      <td>0.414838</td>\n      <td>1.000000</td>\n      <td>0.216225</td>\n      <td>0.018443</td>\n      <td>0.245489</td>\n      <td>0.081629</td>\n    </tr>\n    <tr>\n      <th>Fare</th>\n      <td>0.012658</td>\n      <td>0.096067</td>\n      <td>0.159651</td>\n      <td>0.216225</td>\n      <td>1.000000</td>\n      <td>0.549500</td>\n      <td>0.182333</td>\n      <td>0.257307</td>\n    </tr>\n    <tr>\n      <th>passenger_class</th>\n      <td>0.035144</td>\n      <td>0.369226</td>\n      <td>0.083081</td>\n      <td>0.018443</td>\n      <td>0.549500</td>\n      <td>1.000000</td>\n      <td>0.131900</td>\n      <td>0.338481</td>\n    </tr>\n    <tr>\n      <th>passenger_sex</th>\n      <td>0.042939</td>\n      <td>0.093254</td>\n      <td>0.114631</td>\n      <td>0.245489</td>\n      <td>0.182333</td>\n      <td>0.131900</td>\n      <td>1.000000</td>\n      <td>0.543351</td>\n    </tr>\n    <tr>\n      <th>passenger_survived</th>\n      <td>0.005007</td>\n      <td>0.077221</td>\n      <td>0.035322</td>\n      <td>0.081629</td>\n      <td>0.257307</td>\n      <td>0.338481</td>\n      <td>0.543351</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 162
    }
   ],
   "source": [
    "all_data.corr().abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clasificador Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes:\n",
    "    def __init__(self, train_x, train_y, x_labels=None):\n",
    "        self.x_labels = x_labels\n",
    "        df = self.__init_dataframe(train_x, train_y)\n",
    "        by_class = df.groupby('y')\n",
    "        self.means = by_class.mean()\n",
    "        self.stdvs = by_class.std()\n",
    "        self.cdist = by_class['y'].count() / df.shape[0]\n",
    "\n",
    "    def __init_dataframe(self, train_x, train_y):\n",
    "        df = pd.DataFrame()\n",
    "        df['y'] = train_y\n",
    "        for i,x in enumerate(train_x):\n",
    "            label = self.x_labels[i] if self.x_labels else 'x_' + str(i)\n",
    "            df[label] = x\n",
    "        return df\n",
    "\n",
    "    def predict(self, x_samples):\n",
    "        probs = []\n",
    "        for y in self.cdist.index:\n",
    "            mean = self.means.iloc[y].to_numpy()\n",
    "            std = self.stdvs.iloc[y].to_numpy()\n",
    "            p_x = self.norm_pdf(x_samples, mean, std)\n",
    "            probs.append(self.cdist[y] * np.prod(p_x, axis=1))\n",
    "        probs = np.asarray(probs)\n",
    "        return np.argmax(probs.T,axis=1)\n",
    "\n",
    "    def norm_pdf(self, x, mean, std):\n",
    "        e = np.exp(-((x - mean)**2 / (2 * std**2 )))\n",
    "        return (1 / (sqrt(2*pi) * std)) * e\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clasificador binario basado en regresión logísitca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "  class BinaryLogisticRegression:\n",
    "    def __init__(self, x, y):\n",
    "        # Si x es unidimensional, convertir a columna. \n",
    "        # Si x es matriz, asumir una variable por columna.\n",
    "        self.x = x.reshape(-1,1) if x.ndim == 1 else x \n",
    "        self.y = y.reshape(-1, 1)\n",
    "        self.weights = None\n",
    "        self.epochs = None \n",
    "        self.lr = None\n",
    "        self.print_rate = None\n",
    "        self.feed = None\n",
    "        self.batch_size = None\n",
    "        self.batch_iters = None\n",
    "    \n",
    "    def train(self, epochs, lr, batch_size, print_rate=10):\n",
    "        self.print_rate = print_rate\n",
    "        self.epochs = epochs\n",
    "        self.lr = lr\n",
    "        self.batch_size = min(batch_size, self.x.shape[0])\n",
    "        self.batch_iters = int(self.x.shape[0]/self.batch_size)\n",
    "\n",
    "        g = tf.Graph()\n",
    "        with g.as_default():\n",
    "            return self.__do_train()\n",
    "\n",
    "    \n",
    "    def __do_train(self):\n",
    "        placeholder_x = tf.placeholder(tf.float64, [self.batch_size, self.x.shape[1]], \"x\")\n",
    "        placeholder_y = tf.placeholder(tf.float64, [self.batch_size, self.y.shape[1]], \"y\")\n",
    "\n",
    "        with tf.Session() as session:\n",
    "            (weights_op, error_op) = self.__gradient_descent(placeholder_x, placeholder_y)\n",
    "            self.before_training(session.graph, weights_op, error_op)\n",
    "            session.run(tf.global_variables_initializer())    \n",
    "            for epoch in range(1, self.epochs + 1):\n",
    "                for i in range(self.batch_iters):\n",
    "                    start_index  = i*self.batch_size\n",
    "                    end_index = start_index + self.batch_size\n",
    "\n",
    "                    x_batch = np.array(self.x[start_index:end_index])\n",
    "                    y_batch = np.array(self.y[start_index:end_index])\n",
    "                    self.feed = { placeholder_x: x_batch, placeholder_y: y_batch }\n",
    "\n",
    "\n",
    "                    out = session.run([weights_op, error_op], self.feed)\n",
    "                    self.error = out[1]\n",
    "                    self.weights = out[0]\n",
    "                    self.after_epoch(epoch, session)\n",
    "\n",
    "    def __gradient_descent(self, x, y):\n",
    "        n_samples = x.shape[0]\n",
    "        n_independent_vars = x.shape[1] + 1\n",
    "\n",
    "        bias_feature = tf.ones([n_samples, 1], tf.float64)\n",
    "        x = tf.concat([x, bias_feature], axis=1)\n",
    "\n",
    "        initial_weights = tf.zeros([n_independent_vars, 1], tf.float64)\n",
    "        weights = tf.Variable(name=\"Weights\", initial_value=initial_weights)\n",
    "        logits = tf.matmul(x, weights)\n",
    "        error = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits,labels=y))\n",
    "        gradients = tf.gradients(error, weights)\n",
    "        adjustment = tf.scalar_mul(-self.lr, gradients[0])\n",
    "        weights = tf.assign(weights, (tf.add(weights, adjustment)))\n",
    "        return (weights, error)\n",
    "\n",
    "    def after_epoch(self, epoch, session):\n",
    "        if epoch % self.print_rate == 0 or epoch == self.epochs:\n",
    "            print(\"epoch:\" + str(epoch) + \" error: \" + str(self.error))\n",
    "    \n",
    "    def before_training(self, graph, weights_op, error_op):\n",
    "        # overridear para inicializar tensorboard, etc\n",
    "        pass\n",
    "\n",
    "    def predict(self, x):\n",
    "        x = np.hstack((x,np.ones((x.shape[0],1))))\n",
    "        logits = np.matmul(x, self.weights)\n",
    "        logits_ph = tf.placeholder(tf.float32, logits.shape)\n",
    "        with tf.Session() as session:\n",
    "            feed = {logits_ph: logits}\n",
    "            probs = session.run(tf.nn.sigmoid(logits_ph), feed_dict=feed)\n",
    "            session.close()\n",
    "        y = np.array(list(map(lambda p: 1 if p > 0.5 else 0, probs)))\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# logreg = BinaryLogisticRegression(df[[\"weather_encoded\",\"temp_encoded\"]].to_numpy(), df[\"Y\"].to_numpy())\n",
    "# logreg.train(2000, 0.6, 8, print_rate=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logreg.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scikit Learn DecisionTreeClassifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_decision_tree(x, y):\n",
    "    decision_tree = DecisionTreeClassifier()\n",
    "    decision_tree.fit(x, y)\n",
    "    return decision_tree\n",
    "\n",
    "# gv_data = export_graphviz(decision_tree, out_file=None,\n",
    "#                                         feature_names=[\"weather\",\"temp\"],\n",
    "#                                         class_names=[\"not_play\",\"play\"],\n",
    "#                                         filled=True, rounded=True,\n",
    "#                                         special_characters=True)\n",
    "# graph = graphviz.Source(gv_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_svm(x,y):\n",
    "    svm_classifier = svm.SVC(kernel='linear')\n",
    "    svm_classifier.fit(x,y)\n",
    "    return svm_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38264bitvenvvenv7c8b5f64282a4e90a5603bde2787a492",
   "display_name": "Python 3.8.2 64-bit ('venv': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}