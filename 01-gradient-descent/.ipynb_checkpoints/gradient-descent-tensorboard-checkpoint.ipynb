{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient descent y Tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuración y carga de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "import datetime, os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import math\n",
    "\n",
    "if tf.__version__.startswith(\"2.\"):\n",
    "  import tensorflow.compat.v1 as tf\n",
    "  tf.compat.v1.disable_v2_behavior()\n",
    "  tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "\n",
    "tb_logs_dir = \"./logs/\"\n",
    "os.makedirs(tb_logs_dir, exist_ok=True)\n",
    "\n",
    "%load_ext tensorboard\n",
    "# %tensorboard --logdir {tb_logs_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = np.load('data/proyecto_training_data.npy')\n",
    "\n",
    "training_y = all_data[:,0].reshape(-1,1) / 1000 # precio de venta en miles de dólares\n",
    "training_x = all_data[:,1].reshape(-1,1)\n",
    "n_sample = len(training_y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formulación de hipótesis\n",
    "\n",
    "Verificamos que las variables presentan un coeficiente de correlación positivo y considerablemente alto. Esto nos indica que un modelo de regresión lineal podría ajustarse a los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.7909816]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "corr = tfp.experimental.substrates.numpy.stats.correlation(np.copy(training_x), np.copy(training_y))\n",
    "print(corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definicion de grafo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, training_x, training_y, learning_rate):\n",
    "        with tf.name_scope(\"datos_de_entrenamiento\"):\n",
    "            self.x = tf.concat([training_x, tf.ones_like(training_x)], axis = 1)\n",
    "            self.y = training_y\n",
    "            self.learning_rate = learning_rate\n",
    "        with tf.name_scope(\"pesos\"):\n",
    "            self.weights = tf.Variable(name=\"Weights\", initial_value=tf.zeros((2,1),tf.float64))\n",
    "        with tf.name_scope(\"predecir_y_hat\"):\n",
    "            y_hat = tf.matmul(self.x, self.weights)\n",
    "        with tf.name_scope(\"calcular_error\"):\n",
    "            self.error = (1/2 * tf.reduce_mean(tf.math.square(self.y - y_hat)))\n",
    "        with tf.name_scope(\"calcular_gradientes\"):\n",
    "            gradients = tf.gradients(self.error, self.weights)\n",
    "        with tf.name_scope(\"actualizar_pesos\"):\n",
    "            adjustment = tf.scalar_mul(-self.learning_rate, gradients[0])\n",
    "            self.update = tf.assign(self.weights, (tf.add(self.weights, adjustment)))    \n",
    "        \n",
    "    def epoch(self):\n",
    "        return self.update\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diagrama del grafo\n",
    "![image.png](img/graph.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(learning_rate, epochs, print_rate=200):\n",
    "    print(\"lr=\", str(learning_rate))\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    placeholder_x = tf.placeholder(tf.float64,[n_sample, 1],\"overall_qual\")\n",
    "    placeholder_y = tf.placeholder(tf.float64,[n_sample, 1],\"sale_price\")\n",
    "\n",
    "    with tf.Session() as session:    \n",
    "        feed_dict = {placeholder_x: training_x, placeholder_y: training_y}\n",
    "        model = Model(placeholder_x, placeholder_y, learning_rate)\n",
    "        error_summary = tf.summary.scalar(name='Error', tensor=model.error)\n",
    "\n",
    "        timestamp = datetime.datetime.now().strftime('%Y-%m-%d_%H:%M:%S')\n",
    "        log_name = tb_logs_dir + timestamp + \"_lr=\" + str(learning_rate) + \"_epochs=\" + str(epochs)\n",
    "        writer = tf.summary.FileWriter(log_name, session.graph)\n",
    "\n",
    "        session.run(tf.global_variables_initializer())\n",
    "        \n",
    "        for epoch in range(1, epochs + 1):\n",
    "            session.run(model.epoch(), feed_dict=feed_dict)\n",
    "            writer.add_summary(session.run(error_summary, feed_dict=feed_dict), epoch)\n",
    "            \n",
    "            if epoch % print_rate == 0 or epoch == epochs:\n",
    "                error = session.run(model.error, feed_dict=feed_dict)\n",
    "                print(\"epoch:\" + str(epoch) + \" error: \" + str(error))\n",
    "                if math.isinf(error):\n",
    "                    print(\"terminando por divergencia\")\n",
    "                    break\n",
    "        writer.close()\n",
    "    session.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a utilizar varios valores para el learning_rate hasta encontrar el mejor con la ayuda de tensorboard. Luego intentaremos encontrar una cantidad de iteraciones pequeña pero suficiente para el resultado esperado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr= 0.1\n",
      "epoch:200 error: 3.045890471906833e+195\n",
      "epoch:400 error: inf\n",
      "terminando por divergencia\n",
      "\n",
      "lr= 0.02\n",
      "epoch:200 error: 1350.0802433327196\n",
      "epoch:400 error: 1296.2465977078166\n",
      "epoch:600 error: 1259.4992627900178\n",
      "epoch:800 error: 1234.4151985791357\n",
      "epoch:1000 error: 1217.29258876966\n",
      "\n",
      "lr= 0.01\n",
      "epoch:200 error: 1385.7691204673454\n",
      "epoch:400 error: 1350.0956996448563\n",
      "epoch:600 error: 1320.620919644425\n",
      "epoch:800 error: 1296.2676999070509\n",
      "epoch:1000 error: 1276.1461142135142\n",
      "\n",
      "lr= 0.002\n",
      "epoch:200 error: 1419.6398569870832\n",
      "epoch:400 error: 1410.6833858114394\n",
      "epoch:600 error: 1402.0623140588518\n",
      "epoch:800 error: 1393.7640817856502\n",
      "epoch:1000 error: 1385.77659938947\n",
      "\n",
      "lr= 0.001\n",
      "epoch:200 error: 1424.2494624478513\n",
      "epoch:400 error: 1419.6400747408068\n",
      "epoch:600 error: 1415.119213063067\n",
      "epoch:800 error: 1410.6838050102092\n",
      "epoch:1000 error: 1406.3322353320866\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 1000\n",
    "for learning_rate in [0.1, 0.02, 0.01, 0.002, 0.001]:\n",
    "    train(learning_rate, epochs)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El mejor valor de lr de los experimentos anteriores parece ser 0.02, buscaremos alrededor de ese valor a ver si encontramos uno que reduzca el error más rápido.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr= 0.05\n",
      "epoch:200 error: 67923.49375770157\n",
      "epoch:400 error: 246751.31535903833\n",
      "epoch:600 error: 905760.2772557576\n",
      "epoch:800 error: 3333672.6278908486\n",
      "epoch:1000 error: 12278309.791716497\n",
      "\n",
      "lr= 0.04\n",
      "epoch:200 error: 1296.2043645324281\n",
      "epoch:400 error: 1234.3758482265991\n",
      "epoch:600 error: 1205.5770417331494\n",
      "epoch:800 error: 1192.162984514879\n",
      "epoch:1000 error: 1185.9149154813251\n",
      "\n",
      "lr= 0.03\n",
      "epoch:200 error: 1320.5825991040456\n",
      "epoch:400 error: 1259.4776471274602\n",
      "epoch:600 error: 1225.0208351918222\n",
      "epoch:800 error: 1205.5907914660236\n",
      "epoch:1000 error: 1194.6342758170529\n",
      "\n",
      "lr= 0.025\n",
      "epoch:200 error: 1334.6292623051604\n",
      "epoch:400 error: 1276.1134201203938\n",
      "epoch:600 error: 1239.808642185996\n",
      "epoch:800 error: 1217.2841966400495\n",
      "epoch:1000 error: 1203.309434197787\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for learning_rate in [0.05, 0.04, 0.03, 0.025]:\n",
    "    train(learning_rate, epochs)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que un valor de 0.5 comienza a diverger, mientras que 0.04 no diverge y es el valor que más rápido ha reducido el error. buscando valores entre 0.05 y 0.04 encontramos el que parece ser el ideal: 0.0498"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr= 0.0499\n",
      "epoch:400 error: 11214.003731016739\n",
      "epoch:800 error: 6709.921674005602\n",
      "epoch:1200 error: 4233.737957089572\n",
      "epoch:1600 error: 2867.3257489779458\n",
      "epoch:2000 error: 2112.5488823938686\n",
      "\n",
      "lr= 0.0498\n",
      "epoch:400 error: 1619.3239904480738\n",
      "epoch:800 error: 1194.9179484147107\n",
      "epoch:1200 error: 1181.4896319198906\n",
      "epoch:1600 error: 1180.5945136255332\n",
      "epoch:2000 error: 1180.485616460357\n",
      "\n",
      "lr= 0.0497\n",
      "epoch:400 error: 1233.6084169762803\n",
      "epoch:800 error: 1186.050368049776\n",
      "epoch:1200 error: 1181.3009521433075\n",
      "epoch:1600 error: 1180.592006936388\n",
      "epoch:2000 error: 1180.4858727317653\n",
      "\n",
      "lr= 0.0496\n",
      "epoch:400 error: 1218.433584718377\n",
      "epoch:800 error: 1186.079115242167\n",
      "epoch:1200 error: 1181.310563634387\n",
      "epoch:1600 error: 1180.5939316618885\n",
      "epoch:2000 error: 1180.4862336132403\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for learning_rate in [0.0499, 0.0498, 0.0497, 0.0496]:\n",
    "    train(learning_rate, 2000, print_rate=400)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora buscamos el número óptimo de iteraciones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr= 0.0498\n",
      "epoch:500 error: 1358.608913083541\n",
      "epoch:1000 error: 1183.9314695668368\n",
      "epoch:1500 error: 1180.6763977712426\n",
      "epoch:2000 error: 1180.485616460357\n",
      "epoch:2500 error: 1180.4688857151407\n",
      "epoch:3000 error: 1180.4673432626587\n",
      "epoch:3500 error: 1180.4672003780743\n",
      "epoch:4000 error: 1180.4671871361313\n",
      "epoch:4500 error: 1180.4671859088735\n",
      "epoch:5000 error: 1180.4671857951307\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.0498\n",
    "train(learning_rate, 5_000, print_rate=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Más allá de las 4000 iteraciones, la ganancia es marginalmente más pequeña cada vez. Vemos que a partir de las 1500 iteraciones, el error se estabiliza alrededor de 1180. Más allá de las 2500 iteraciones, la mejora es marginalmente más pequeña cada vez. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### lr=0.1 y lr=.0.05hacen que el error diverja, mientras que lr=0.49 lo hace converger\n",
    "![img/01.jpg](img/01.png)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Al analizar las primeras 80 iteraciones, vemos que un lr más bajo causa un error inicial más bajo, pero también una velocidad de convergencia más lenta, como podemos ver al analizar más iteraciones\n",
    "![image.png](img/02.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusiones\n",
    "Al evaluar las gráficas y los datos numéricos por más iteraciones, notamos que aunque el error inicial es mucho más pequeño con lr=0.01, el valor de lr=0.0498 se acerca al valor límite a una velocidad mucho más rápida, por lo que tiene la ventaja de requerir menos iteraciones para entrenarse, siempre y cuando le de tiempo de compensar el error alto en las primeras 600 o 700 iteraciones aproximadamente.\n",
    "\n",
    "Después de unas 4000 iteraciones, el lr=0.01 llega a acercarse bastante al otro, pero incluso tras 10,000 iteraciones, el learning rate más alto todavía logra un valor numérico más bajo del error por algunos decimales. Lo cual posiblemente ya no sea significativo para la aplicación real, aunque al ser una cantidad en dólares al cuadrado, puede ser que sí interese llegar al valor más bajo posible. En ese caso, se seleccionaría el experimento con **lr=0.498 y 4000 a 5000 iteraciones** como el mejor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](img/03.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
